{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Neural Network?\n",
    "\n",
    "Neural networks are the overarching idea behind deep learning. By adjusting a set of weights and biases to fit itself to a dataset, neural networks are extremely versatile and can find patterns in many different types of data. Neural networks can also be built to specification, with different types networks and fine-tuning built for use cases such as image classification, time series analysis, and more.\n",
    "\n",
    "## Parts of a neural network\n",
    "\n",
    "Often when people try to explain a neural network, they show this image:\n",
    "\n",
    "[conventional digram of neural network]\n",
    "\n",
    "However, this diagram does not really make mathematical sense (nor does it make much sense from a programming perspective either). Thus, we shall begin with this diagram:\n",
    "\n",
    "``` \n",
    "           f_1(X)      f_2(X)             f_N(X)\n",
    "          +------+    +------+           +------+    \n",
    "input     |      |    |      |           |      |  output  \n",
    "--------> |      | -> |      | -> ... -> |      | --------> prediction \n",
    "          |      |    |      |           |      | \n",
    "          +------+    +------+           +------+    \n",
    "```\n",
    "\n",
    "The diagram shows that at its core, a neural network is just a composition of functions. These functions typically have the form \n",
    "\n",
    "$\\overrightarrow{y} = f_i(\\overrightarrow{x}; \\theta_1, ..., \\theta_n; h_1, ..., h_m)$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\overrightarrow{x}$: input vector of values\n",
    "- $\\overrightarrow{y}$: output vector of values\n",
    "- $\\theta_i$: trainable parameter (i.e., weight or bias)\n",
    "- $h_i$: hyperparameter (non-trainable parameter which affects the behaviour of the neural network)\n",
    "- $n \\geq{0}, m \\geq{0}$\n",
    "\n",
    "This is the bare essentials of a neural network. We are, however, missing some other essential parts which actually allow the neural network to actually get better at making predictions. Now we can refine this diagram a bit without loss of generality:\n",
    "\n",
    "``` \n",
    "           f_1(X)      f_2(X)             f_N(X)\n",
    "          +------+    +------+           +------+  output y \n",
    "input     |      | -> |      | -> ... -> |      | ---------->        comparison \n",
    "--------> |      |    |      |           |      |                vs. expected output\n",
    "          |      | <- |      | <- ... <- |      | <----------         E(y, y')\n",
    "          +------+    +------+           +------+   error O\n",
    "           b_1(O)      b_2(O)             b_N(O) \n",
    "```\n",
    "\n",
    "We have now added a few functions which give a better picture of how the neural network works. Once we use the forward functions $f_i$ to get a prediction (this process is known as feed-forward or forward propagation), we compare it against our expected output. We can then get an error metric using this comparison, denoted by a function $E$. This error is then passed back through the neural network using the backwards functions $b_i$ (this process is known as back propagation) to update the trainable parameters.\n",
    "\n",
    "We now have all the components of a neural network. But this is all very abstract. How do we actually define the functions $f_i$, $b_i$, and $R$? That is usecase-specific. In fact, we can build neural networks as we wish. We refer to each function $f_i$ as a layer of the neural network. Each layer has a corresponding backpropagation function $b_i$ which is defined based on the forward propagation function. With these definitions, we can now begin to describe different classes of neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN/FNN\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The Artificial Neural Network (ANN), also known as the feed-forward neural network (FNN), is the simplest type of neural network. It is also the one pictured in a conventional diagram like this one:\n",
    "\n",
    "[conventional diagram of neural network]\n",
    "\n",
    "An ANN is comprised of one or more layers, where each layer is comprised of one or more nodes, or neurons. Each neuron has a set of weights and biases, and produces an output, known as the neuron's activation, based on its weights and biases. Layers have the following form: \n",
    "\n",
    "$f_i(\\overrightarrow{x})$ = [$\\sigma(\\overrightarrow{x} \\cdot \\overrightarrow{w_j} + b_j)$ for each node $j$ in layer $i$] $\\in \\mathbb{R}^m$\n",
    "\n",
    "Such that:\n",
    "\n",
    "- $\\overrightarrow{x}$: input vector of values\n",
    "- $\\overrightarrow{w}$: vector of trainable weights\n",
    "- $b$: trainable bias parameter\n",
    "- $\\sigma$: activation function which transforms the final value, altering the behaviour of the neural network\n",
    "- $\\overrightarrow{x}, \\overrightarrow{w} \\in \\mathbb{R}^n$ where n is the input size\n",
    "- output is a vector in $\\mathbb{R}^m$\n",
    "\n",
    "The input to $f_i$ is the result of $f_{i-1}$, that is, the output of all nodes from the previous layer. The first layer, $f_1$, is referred to as the input layer, while the last layer, $f_{N}$, is the output layer. All other layers are are referred to as hidden layers of the ANN.\n",
    "\n",
    "The layers of the neural network are **densely** connected- that is, each neuron from a layer receives input from **all** neurons in the layer before it, and propagates its output to **all** neurons in the layer after it. The following diagrams summarize the neuron and ANN architecture:\n",
    "\n",
    "[diagram of node]\n",
    "\n",
    "[detailed diagram of ANN]\n",
    "\n",
    "### Specifics\n",
    "\n",
    "So with that simple overview, we are ready to explain how an ANN actually works. This is the most important neural network to understand as all others are essentially variations on this one. \n",
    "\n",
    "We will walk through an example, and explain all the math behind it.\n",
    "\n",
    "### Definition\n",
    "\n",
    "We must first define our neural network. Consider the following structure:\n",
    "\n",
    "```\n",
    "input layer: 3 neurons\n",
    "hidden layer 1: 5 neurons\n",
    "hidden layer 2: 4 neurons\n",
    "output layer: 4 neurons\n",
    "```\n",
    "\n",
    "We know the size of each layer, but is that enough to get started? No. We also need to pick out an activation function $\\sigma$ for each layer. Activation function are generally one of the following for an ANN, and we choose based on which one we think could perform well on our dataset. A full list of common activation functions can be found [here](https://www.v7labs.com/blog/neural-networks-activation-functions) but for ANNs we usually pick one or more of these:\n",
    "\n",
    "- sigmoid: $\\sigma(x) = \\frac{1}{1 + e^{-x}}$, commonly used when the output is a probability e.g., in classification problems (because we want the probability that an input belongs to a given class as output)\n",
    "- TanH: $\\sigma(x) = \\frac{(e^{x}-e^{-x})}{(e^{x}+e^{-x})}$, steeper version of sigmoid.\n",
    "- ReLU (Rectified Linear Unit): $\\sigma(x) = max(x, 0)$, commonly used for images where we don't want all neurons to be activated (have a high output) at the same time.\n",
    "\n",
    "Hidden layers typically all use the same activation function, while the output layer may use a different one to correctly translate the value. However, the correct activation function to use depends heavily on the type of data you are using. For example, if we are solving a classification problem on a set of images, we may use ReLU for the hidden layers, then sigmoid for the output layer as the output *must* be between 0 and 1. We select activation functions as follows for our example (note: the selections for this example are arbitrary):\n",
    "\n",
    "```\n",
    "input layer: 3 neurons, ReLU\n",
    "hidden layer 1: 5 neurons, sigmoid\n",
    "hidden layer 2: 4 neurons, sigmoid\n",
    "output layer: 4 neurons, TanH\n",
    "```\n",
    "\n",
    "Are we done yet? No!\n",
    "\n",
    "We also need to choose a loss or error function $E$ for our neural network. This function will calculate the difference between our output and the expected output, known as loss or cost (or error). This function will also affect how our neural network is trained. A full list of common loss functions can be found [here](https://www.v7labs.com/blog/neural-networks-activation-functions) but for ANNs we usually pick one of these:\n",
    "\n",
    "- Mean Squared Error (MSE): $\\frac{1}{n}\\sum^{n}_{i = 1}(e_i-a_i)^2$, punishes high differences between output and expected output\n",
    "- Cross Entropy: $-\\frac{1}{n}\\sum^{n}_{i = 1}e_iln(a_i)^2$, used for classification to heavily punish incorrect answers\n",
    "\n",
    "The right loss function to use also depends heavily on the type of problem you are trying to solve, in this case we will aribtrarily choose one for demonstration purposes.\n",
    "\n",
    "```\n",
    "input layer: 3 neurons, ReLU\n",
    "hidden layer 1: 5 neurons, sigmoid\n",
    "hidden layer 2: 4 neurons, sigmoid\n",
    "output layer: 4 neurons, TanH\n",
    "Loss: Mean Squared Error\n",
    "```\n",
    "\n",
    "Now, we have successfully defined a neural network!\n",
    "\n",
    "### Initialization\n",
    "\n",
    "The second step is to initialize the neural network, that is, set values for its weights and biases. These can be random, or determinate (e.g. 0). In our case, let's set our weights to be random normally distributed numbers.\n",
    "\n",
    "```\n",
    "input layer: {w: [0, 0, 0], b: [0, 0, 0]}\n",
    "hidden layer 1: {w: [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], b: [0, 0, 0, 0, 0]}\n",
    "hidden layer 2: {w: [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], b: [0, 0, 0, 0]}\n",
    "output layer: {w: [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], b: [0, 0, 0, 0]}\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
